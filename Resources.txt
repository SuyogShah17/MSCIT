--------resources :-----------
option  1:-- 
login google classrooom for all resources of prac 1,2,3


option 2:--
Prac1a,b..all : multiple transform on one image (download any from internet
prac2 : two overlaping images of panaroma
prac3 : many images of chess board
prac4 : 
a - need image of face & haarcascade_frontalface_default.xml -this file
b1 - stop sign image & stop_data.xml
b2 - image of pedestrain walking
b3 - two img lena1, lena2 & dlib-19.22.99-cp310-cp310-win_amd64.whl
4c - image of people walking 
4d - image of someones face & dlib-19.22.99-cp310-cp310-win_amd64.whl
prac 5 








-----Prac1 all---
import cv2
import numpy as np

# Prac1: Load image from local file
img = cv2.imread(r'E:\sem2\practicals\1.jpg')
if img is None:
    print("Image not found.")
    exit()

cv2.imshow("Original", img)
cv2.waitKey(0)

rows, cols = img.shape[:2]

transforms = {
    "Prac1A: Translated": cv2.warpAffine(img, np.float32([[1, 0, 100], [0, 1, 50]]), (cols, rows)),
    "Prac1B: Scaled": cv2.resize(img, None, fx=1.5, fy=1.5),
    "Prac1C: Shrunk": cv2.resize(img, None, fx=0.5, fy=0.5),
    "Prac1D: Rotated": cv2.warpAffine(img, cv2.getRotationMatrix2D((cols/2, rows/2), 45, 1), (cols, rows)),
    "Prac1G: Sheared X": cv2.warpAffine(img, np.float32([[1, 0.5, 0], [0, 1, 0]]), (int(cols * 1.5), rows)),
    "Prac1H: Sheared Y": cv2.warpAffine(img, np.float32([[1, 0, 0], [0.5, 1, 0]]), (cols, int(rows * 1.5))),
    "Prac1I: Reflected": cv2.flip(img, 1),
    "Prac1J: Cropped": img[50:200, 100:300]
}

for name, result in transforms.items():
    cv2.imshow(name, result)
    if cv2.waitKey(0) == 27:  # Esc key to exit early
        break
    cv2.destroyAllWindows()

cv2.destroyAllWindows()


------- Practical 2: Image Stitching ------------

import cv2, matplotlib.pyplot as plt
img1 = cv2.imread(r"E:\sem2\practicals\uttor_left.jpg")
img2 = cv2.imread(r"E:\sem2\practicals\uttor_right.jpg") # Load images

if img1 is None or img2 is None:
    print("Error loading images. Check paths:\n",
          r"E:\sem2\practicals\uttor_left.jpg", "\n",
          r"E:\sem2\practicals\uttor_right.jpg")
else:
    # Corrected indentation (4 spaces)
    s = cv2.Stitcher_create()
    stat, stitched = s.stitch((img1, img2))
    if stat == cv2.Stitcher_OK:
        stitched_rgb = cv2.cvtColor(stitched, cv2.COLOR_BGR2RGB)
        plt.imshow(stitched_rgb); plt.title('Stitched Image'); plt.axis('off'); plt.show()
    else:
        print(f"Stitching failed: {stat}. Ensure images overlap sufficiently.")

-----------Practical 3-------------
import cv2, glob, numpy as np
# Calibration parameters and chessboard dimensions
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
dims = (7, 6)
objp = np.zeros((dims[0]*dims[1], 3), np.float32)
objp[:, :2] = np.mgrid[0:dims[0], 0:dims[1]].T.reshape(-1, 2)
objpts, imgpts = [], []
# Process all JPEG images in the folder
for f in glob.glob("*.jpg"):
    img = cv2.imread(f)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    ret, corners = cv2.findChessboardCorners(gray, dims, None)
    if ret:
        objpts.append(objp)
        corners = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
        imgpts.append(corners)
        cv2.drawChessboardCorners(img, dims, corners, ret)
        cv2.imshow("Calib", img); cv2.waitKey(500)
cv2.destroyAllWindows()
ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpts, imgpts, gray.shape[::-1], None, None)
print("Camera Matrix:\n", mtx)
print("Distortion Coefficients:\n", dist)

--------------------------------prac 4-----------------------------
# Practical 4-A to 4-D | Performed by Suyog Shah
import cv2, numpy as np, face_recognition as fr, imutils
from matplotlib import pyplot as plt

# 4-A: Face Detection
f=cv2.CascadeClassifier("E:/sem2/practicals/haarcascade_frontalface_default.xml")
i=cv2.imread("E:/sem2/practicals/input_image2.jpg");g=cv2.cvtColor(i,cv2.COLOR_BGR2GRAY)
for(x,y,w,h)in f.detectMultiScale(g,1.1,5,minSize=(40,40)):cv2.rectangle(i,(x,y),(x+w,y+h),(0,255,0),4)
plt.imshow(cv2.cvtColor(i,cv2.COLOR_BGR2RGB));plt.title("4-A");plt.show()

# 4-B-i: Object Detection (Stop sign or custom XML)
c=cv2.CascadeClassifier("E:/sem2/practicals/stop_data.xml")
i=cv2.imread("E:/sem2/practicals/image.jpg");g=cv2.cvtColor(i,cv2.COLOR_BGR2GRAY)
d=c.detectMultiScale(g,minSize=(30,30));print("Detected:",len(d))
for(x,y,w,h)in d:cv2.rectangle(i,(x,y),(x+w,y+h),(0,255,0),2)
plt.imshow(cv2.cvtColor(i,cv2.COLOR_BGR2RGB));plt.title("4-B-i");plt.show()

# 4-B-ii: Line Detection
i=cv2.imread("E:/sem2/practicals/lines.jpg")
g,e=cv2.cvtColor(i,cv2.COLOR_BGR2GRAY),cv2.Canny(i,50,150,apertureSize=3)
l=cv2.HoughLinesP(e,1,np.pi/100,100,minLineLength=3,maxLineGap=5)
if l is not None:
 for pt in l: x1,y1,x2,y2=pt[0];cv2.line(i,(x1,y1),(x2,y2),(0,255,0),2)
cv2.imshow("4-B-ii",i);cv2.waitKey(0)

# 4-B-iii: Circle Detection
i=cv2.imread("E:/sem2/practicals/coins.jpg");g=cv2.cvtColor(i,cv2.COLOR_BGR2GRAY)
b=cv2.blur(g,(3,3))
c=cv2.HoughCircles(b,cv2.HOUGH_GRADIENT,1,20,param1=50,param2=30,minRadius=1,maxRadius=35)
if c is not None:
 c=np.uint16(np.around(c))
 for pt in c[0,:]: a,b,r=pt;cv2.circle(i,(a,b),r,(0,255,0),2)
cv2.imshow("4-B-iii",i);cv2.waitKey(0)

# 4-C: Pedestrian Detection
hog=cv2.HOGDescriptor();hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
i=cv2.imread("E:/sem2/practicals/pedestrian.jpg");i=imutils.resize(i,width=min(400,i.shape[1]))
for(x,y,w,h)in hog.detectMultiScale(i,winStride=(4,4),padding=(4,4),scale=1.05)[0]:cv2.rectangle(i,(x,y),(x+w,y+h),(0,0,255),2)
cv2.imshow("4-C",i);cv2.waitKey(0)

# 4-D: Face Recognition | Performed by Suyog Shah
import cv2, face_recognition as fr
i1=fr.load_image_file("E:/sem2/practicals/input.png");i2=fr.load_image_file("E:/sem2/practicals/input.png")
e1,e2=fr.face_encodings(i1)[0],fr.face_encodings(i2)[0]
print("4-D Match:",fr.compare_faces([e1],e2)[0])  # Practical 4-D
for i,l,c in zip([i1,i2],[fr.face_locations(i1)[0],fr.face_locations(i2)[0]],[(0,255,0),(0,0,255)]):
 i=cv2.rectangle(i,(l[3],l[0]),(l[1],l[2]),c,2);cv2.imshow("4-D",cv2.cvtColor(i,cv2.COLOR_RGB2BGR))
cv2.waitKey(0);cv2.destroyAllWindows()


--------------prac5---------------
# main.py  (file1) :
import cv2; from tracker import EuclideanDistTracker
print("Practical Performed By Suyog Shah")
cap = cv2.VideoCapture(r"E:\sem2\practicals\journals\highway.mp4")
tracker = EuclideanDistTracker(); object_detector = cv2.createBackgroundSubtractorMOG2(100, 60)

while True:
    ret, frame = cap.read(); 
    if not ret: break
    roi = frame[340:720, 500:800]
    mask = object_detector.apply(roi)
    _, mask = cv2.threshold(mask, 254, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    detections = [(x, y, w, h) for cnt in contours if cv2.contourArea(cnt) > 100 for x, y, w, h in [cv2.boundingRect(cnt)]]
    for x, y, w, h in detections: cv2.rectangle(roi, (x, y), (x+w, y+h), (0, 255, 0), 2)
    for x, y, w, h, id in tracker.update(detections):
        cv2.putText(roi, str(id), (x, y-15), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 2)
        cv2.rectangle(roi, (x, y), (x+w, y+h), (0, 255, 0), 3)
    cv2.imshow("ROI", roi); cv2.imshow("Frame", frame)
    if cv2.waitKey(30) == 27: break

cap.release(); cv2.destroyAllWindows()

# tracker.py (file2) : 
import math
class EuclideanDistTracker:
    def __init__(self):
        self.center_points = {}; self.id_count = 0
    def update(self, objects_rect):
        objects_bbs_ids = []
        new_center_points = {}
        for x, y, w, h in objects_rect:
            cx, cy = x + w // 2, y + h // 2
            same_object_detected = False
            for id, pt in self.center_points.items():
                if math.hypot(cx - pt[0], cy - pt[1]) < 25:
                    new_center_points[id] = (cx, cy)
                    objects_bbs_ids.append([x, y, w, h, id])
                    same_object_detected = True
                    break
            if not same_object_detected:
                new_center_points[self.id_count] = (cx, cy)
                objects_bbs_ids.append([x, y, w, h, self.id_count])
                self.id_count += 1
        self.center_points = new_center_points.copy()
        return objects_bbs_ids
