--------resources :-----------
option  1:-- 
login google classrooom for all resources of prac 1,2,3


option 2:--

ðŸ§© Practical 1 (Geometric Transformations)
one image for transformations

ðŸ§© Practical 2 (Image Stitching)
Needs overlapping views:
left01.jpg to left14.jpg â€” Good set for stitching
Resultant_Stitched_Panorama.jpg â€” Final output image
Faces1.jpg â€” Optional matchable image if used

ðŸ§© Practical 3 (Camera Calibration)
Needs chessboard images:
uttor_left.jpg, uttor_right.jpg
calibresult.jpg â€” Final result image (optional save)
Folder possibly used: calibresult/, building/, align/

ðŸ§© Practical 4-A (Face Detection)
face.jpg (probably saved as): use â†’ Faces1.jpg, input_image2.jpg, or recognized.jpg
haarcascade_frontalface_default.xml â€” Needed

ðŸ§© Practical 4-B-i (Object Detection with Custom XML)
Image: STOP.jpg, Stop_Img.jpg
XML: stop_data.xml

ðŸ§© Practical 4-B-ii (Line Detection)
lines.jpg, highway.jpg

ðŸ§© Practical 4-B-iii (Circle Detection)
coins.jpg

ðŸ§© Practical 4-C (Pedestrian Detection)
pedestrian.jpg

ðŸ§© Practical 4-D (Face Recognition)
Known: recognized.jpg
Unknowns: recognized_1.jpg, recognized_2.jpg

ðŸ§© Practical 5-A (Tracking)
tracking_input.mp4 â€” Likely your tracker/ folder or file inside it

ðŸ§© Practical 5-B (Face Counting)
Same faces from recognized.jpg, or video in Resources/ or Faces1.jpg
people_video.mp4 â€” Likely inside Resources/ or bda/

ðŸ§© Practical 6 (Image Colorization)
Image: gray.jpg (not listed â€” might be lena.jpg used as grayscale)
Files:
colorization_deploy_v2.prototxt
colorization_release_v2.caffemodel
pts_in_hull.npy

ðŸ§© Practical 7 (Text Detection + OCR)
STOP.jpg, testing.jpg, recognized.jpg â€” Can all be used for text
Install Tesseract separately

ðŸ§© Practical 8 (3D from Stereo Images)
cube1.jpg, cube2.jpg

ðŸ§© Practical 9 (Feature Matching with RANSAC)
Similar views:
recognized_1.jpg vs recognized_2.jpg
left01.jpg vs left02.jpg (if good features exist)



-----Prac1 all---
import cv2
import numpy as np

# Prac1: Load image from local file
img = cv2.imread(r'E:\sem2\practicals\1.jpg')
if img is None:
    print("Image not found.")
    exit()

cv2.imshow("Original", img)
cv2.waitKey(0)

rows, cols = img.shape[:2]

transforms = {
    "Prac1A: Translated": cv2.warpAffine(img, np.float32([[1, 0, 100], [0, 1, 50]]), (cols, rows)),
    "Prac1B: Scaled": cv2.resize(img, None, fx=1.5, fy=1.5),
    "Prac1C: Shrunk": cv2.resize(img, None, fx=0.5, fy=0.5),
    "Prac1D: Rotated": cv2.warpAffine(img, cv2.getRotationMatrix2D((cols/2, rows/2), 45, 1), (cols, rows)),
    "Prac1G: Sheared X": cv2.warpAffine(img, np.float32([[1, 0.5, 0], [0, 1, 0]]), (int(cols * 1.5), rows)),
    "Prac1H: Sheared Y": cv2.warpAffine(img, np.float32([[1, 0, 0], [0.5, 1, 0]]), (cols, int(rows * 1.5))),
    "Prac1I: Reflected": cv2.flip(img, 1),
    "Prac1J: Cropped": img[50:200, 100:300]
}

for name, result in transforms.items():
    cv2.imshow(name, result)
    if cv2.waitKey(0) == 27:  # Esc key to exit early
        break
    cv2.destroyAllWindows()

cv2.destroyAllWindows()


------- Practical 2: Image Stitching ------------

import cv2, matplotlib.pyplot as plt
img1 = cv2.imread(r"E:\sem2\practicals\uttor_left.jpg")
img2 = cv2.imread(r"E:\sem2\practicals\uttor_right.jpg") # Load images

if img1 is None or img2 is None:
    print("Error loading images. Check paths:\n",
          r"E:\sem2\practicals\uttor_left.jpg", "\n",
          r"E:\sem2\practicals\uttor_right.jpg")
else:
    # Corrected indentation (4 spaces)
    s = cv2.Stitcher_create()
    stat, stitched = s.stitch((img1, img2))
    if stat == cv2.Stitcher_OK:
        stitched_rgb = cv2.cvtColor(stitched, cv2.COLOR_BGR2RGB)
        plt.imshow(stitched_rgb); plt.title('Stitched Image'); plt.axis('off'); plt.show()
    else:
        print(f"Stitching failed: {stat}. Ensure images overlap sufficiently.")

-----------Practical 3-------------
import cv2, glob, numpy as np
# Calibration parameters and chessboard dimensions
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)
dims = (7, 6)
objp = np.zeros((dims[0]*dims[1], 3), np.float32)
objp[:, :2] = np.mgrid[0:dims[0], 0:dims[1]].T.reshape(-1, 2)
objpts, imgpts = [], []
# Process all JPEG images in the folder
for f in glob.glob("*.jpg"):
    img = cv2.imread(f)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    ret, corners = cv2.findChessboardCorners(gray, dims, None)
    if ret:
        objpts.append(objp)
        corners = cv2.cornerSubPix(gray, corners, (11, 11), (-1, -1), criteria)
        imgpts.append(corners)
        cv2.drawChessboardCorners(img, dims, corners, ret)
        cv2.imshow("Calib", img); cv2.waitKey(500)
cv2.destroyAllWindows()
ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpts, imgpts, gray.shape[::-1], None, None)
print("Camera Matrix:\n", mtx)
print("Distortion Coefficients:\n", dist)

--------------------------------prac 4-----------------------------
# Practical 4-A to 4-D | Performed by Suyog Shah
import cv2, numpy as np, face_recognition as fr, imutils
from matplotlib import pyplot as plt

# 4-A: Face Detection
f=cv2.CascadeClassifier("E:/sem2/practicals/haarcascade_frontalface_default.xml")
i=cv2.imread("E:/sem2/practicals/input_image2.jpg");g=cv2.cvtColor(i,cv2.COLOR_BGR2GRAY)
for(x,y,w,h)in f.detectMultiScale(g,1.1,5,minSize=(40,40)):cv2.rectangle(i,(x,y),(x+w,y+h),(0,255,0),4)
plt.imshow(cv2.cvtColor(i,cv2.COLOR_BGR2RGB));plt.title("4-A");plt.show()

# 4-B-i: Object Detection (Stop sign or custom XML)
c=cv2.CascadeClassifier("E:/sem2/practicals/stop_data.xml")
i=cv2.imread("E:/sem2/practicals/image.jpg");g=cv2.cvtColor(i,cv2.COLOR_BGR2GRAY)
d=c.detectMultiScale(g,minSize=(30,30));print("Detected:",len(d))
for(x,y,w,h)in d:cv2.rectangle(i,(x,y),(x+w,y+h),(0,255,0),2)
plt.imshow(cv2.cvtColor(i,cv2.COLOR_BGR2RGB));plt.title("4-B-i");plt.show()

# 4-B-ii: Line Detection
i=cv2.imread("E:/sem2/practicals/lines.jpg")
g,e=cv2.cvtColor(i,cv2.COLOR_BGR2GRAY),cv2.Canny(i,50,150,apertureSize=3)
l=cv2.HoughLinesP(e,1,np.pi/100,100,minLineLength=3,maxLineGap=5)
if l is not None:
 for pt in l: x1,y1,x2,y2=pt[0];cv2.line(i,(x1,y1),(x2,y2),(0,255,0),2)
cv2.imshow("4-B-ii",i);cv2.waitKey(0)

# 4-B-iii: Circle Detection
i=cv2.imread("E:/sem2/practicals/coins.jpg");g=cv2.cvtColor(i,cv2.COLOR_BGR2GRAY)
b=cv2.blur(g,(3,3))
c=cv2.HoughCircles(b,cv2.HOUGH_GRADIENT,1,20,param1=50,param2=30,minRadius=1,maxRadius=35)
if c is not None:
 c=np.uint16(np.around(c))
 for pt in c[0,:]: a,b,r=pt;cv2.circle(i,(a,b),r,(0,255,0),2)
cv2.imshow("4-B-iii",i);cv2.waitKey(0)

# 4-C: Pedestrian Detection
hog=cv2.HOGDescriptor();hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())
i=cv2.imread("E:/sem2/practicals/pedestrian.jpg");i=imutils.resize(i,width=min(400,i.shape[1]))
for(x,y,w,h)in hog.detectMultiScale(i,winStride=(4,4),padding=(4,4),scale=1.05)[0]:cv2.rectangle(i,(x,y),(x+w,y+h),(0,0,255),2)
cv2.imshow("4-C",i);cv2.waitKey(0)

# Practical 4-D + 5-B | Face Detection (Image) + Face Count (Video) using face_recognition
import cv2, face_recognition as fr
cap=cv2.VideoCapture("E:/sem2/practicals/people_video.mp4")  # Face video input
while cap.isOpened():
    ret, f = cap.read()
    if not ret: break    
    # ---- 4-D: Face Detection (via face_recognition) ----
    rgb = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)
    faces = fr.face_locations(rgb)    
    # ---- 5-B: Face Counting & Drawing Rectangles ----
    for (t, r, b, l) in faces:
        cv2.rectangle(f, (l, t), (r, b), (0, 255, 0), 2)
    cv2.putText(f, f"Faces: {len(faces)}", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)    
    cv2.imshow("4-D + 5-B Face Count", f)
    if cv2.waitKey(1) == 27: break  # Esc to exit
cap.release(); cv2.destroyAllWindows()


--------------prac5a---------------
# main.py  (file1) :
import cv2; from tracker import EuclideanDistTracker
print("Practical Performed By Suyog Shah")
cap = cv2.VideoCapture(r"E:\sem2\practicals\journals\highway.mp4")
tracker = EuclideanDistTracker(); object_detector = cv2.createBackgroundSubtractorMOG2(100, 60)

while True:
    ret, frame = cap.read(); 
    if not ret: break
    roi = frame[340:720, 500:800]
    mask = object_detector.apply(roi)
    _, mask = cv2.threshold(mask, 254, 255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    detections = [(x, y, w, h) for cnt in contours if cv2.contourArea(cnt) > 100 for x, y, w, h in [cv2.boundingRect(cnt)]]
    for x, y, w, h in detections: cv2.rectangle(roi, (x, y), (x+w, y+h), (0, 255, 0), 2)
    for x, y, w, h, id in tracker.update(detections):
        cv2.putText(roi, str(id), (x, y-15), cv2.FONT_HERSHEY_PLAIN, 1, (255, 0, 0), 2)
        cv2.rectangle(roi, (x, y), (x+w, y+h), (0, 255, 0), 3)
    cv2.imshow("ROI", roi); cv2.imshow("Frame", frame)
    if cv2.waitKey(30) == 27: break

cap.release(); cv2.destroyAllWindows()

# tracker.py (file2) : 
import math
class EuclideanDistTracker:
    def __init__(self):
        self.center_points = {}; self.id_count = 0
    def update(self, objects_rect):
        objects_bbs_ids = []
        new_center_points = {}
        for x, y, w, h in objects_rect:
            cx, cy = x + w // 2, y + h // 2
            same_object_detected = False
            for id, pt in self.center_points.items():
                if math.hypot(cx - pt[0], cy - pt[1]) < 25:
                    new_center_points[id] = (cx, cy)
                    objects_bbs_ids.append([x, y, w, h, id])
                    same_object_detected = True
                    break
            if not same_object_detected:
                new_center_points[self.id_count] = (cx, cy)
                objects_bbs_ids.append([x, y, w, h, self.id_count])
                self.id_count += 1
        self.center_points = new_center_points.copy()
        return objects_bbs_ids


---------------prac 6---------------
import cv2
img = cv2.imread(r'E:\sem2\practicals\1.jpg')
if img is None:
    print("Image not found.")
    exit()
cv2.imshow("Original Image", img)
# Convert Color Image to Grayscale
gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
cv2.imshow("Grayscale Image", gray_img)
# Convert Grayscale Image to Color
color_img = cv2.applyColorMap(gray_img, cv2.COLORMAP_JET)
cv2.imshow("Colorized Image", color_img)
cv2.waitKey(0)
cv2.destroyAllWindows()

---------prac7----------
import cv2, pytesseract
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
img = cv2.imread(r"E:\sem2\practicals\22.jpg")
if img is None: exit("Load Error")
img = cv2.resize(img, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)
gray = cv2.equalizeHist(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY))
_, th = cv2.threshold(gray, 0, 255, cv2.THRESH_OTSU | cv2.THRESH_BINARY_INV)
d = cv2.getStructuringElement(cv2.MORPH_RECT, (18, 18))
dil = cv2.dilate(th, d, iterations=1)
cnts, _ = cv2.findContours(dil, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
img2 = img.copy(); cfg = r'--oem 3 --psm 6'
with open(r"E:\sem2\practicals\recognized_2.txt", "w") as f:
    for c in cnts:
        x, y, w, h = cv2.boundingRect(c)
        f.write(pytesseract.image_to_string(img2[y:y+h, x:x+w], config=cfg).strip() + "\n")
        cv2.rectangle(img2, (x, y), (x+w, y+h), (0, 255, 0), 2)
print("OCR results saved")
cv2.imshow("Processed", img2); cv2.waitKey(0); cv2.destroyAllWindows()


-----------prac 8------------
from PIL import Image
import numpy as np, os

def shift_image(img, dimg, s=10):
    data = np.array(img.convert("RGBA"))
    delta = ((np.array(dimg.convert("L")) / 255) * s).astype(int)
    sh = np.zeros_like(data)
    h, w, _ = data.shape
    for y, row in enumerate(delta):
        for x, dx in enumerate(row):
            if 0 <= x + dx < w:
                sh[y, x + dx] = data[y, x]
    return Image.fromarray(sh.astype(np.uint8))

print("Practical Performed By Suyog Shah")
ip, dp = r"E:\sem2\practicals\cube1.jpeg", r"E:\sem2\practicals\cube2.jpeg"
if os.path.exists(ip) and os.path.exists(dp):
    img, dimg = Image.open(ip), Image.open(dp)
    shift_image(img, dimg, 10).show()
else:
    print("Error: one or both image files are missing.")


-----------prac 9-----------
import cv2, numpy as np
print("Practical Performed By Suyog Shah")
i1 = cv2.imread("align.jpg"); i2 = cv2.imread("ref.jpg")
if i1 is None or i2 is None: exit("Error: Could not load images.")
g1 = cv2.cvtColor(i1, cv2.COLOR_BGR2GRAY); g2 = cv2.cvtColor(i2, cv2.COLOR_BGR2GRAY)
orb = cv2.ORB_create(5000)
kp1, d1 = orb.detectAndCompute(g1, None); kp2, d2 = orb.detectAndCompute(g2, None)
ms = sorted(cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True).match(d1, d2), key=lambda m: m.distance)
ms = ms[:int(len(ms) * 0.9)]
p1 = np.float32([kp1[m.queryIdx].pt for m in ms]); p2 = np.float32([kp2[m.trainIdx].pt for m in ms])
H, _ = cv2.findHomography(p1, p2, cv2.RANSAC)
aligned = cv2.warpPerspective(i1, H, (g2.shape[1], g2.shape[0]))
cv2.imwrite("output.jpg", aligned)
cv2.imshow("Aligned Image", aligned); cv2.waitKey(0); cv2.destroyAllWindows()
















