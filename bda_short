---------------------------- Prac 1 ---------------------------------------------
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

data = pd.get_dummies(pd.read_csv("E:/sem2/practicals/bda/Wholesalecustomersdata.csv"), columns=['Channel', 'Region'])
data_scaled = MinMaxScaler().fit_transform(data)

plt.plot(range(1, 15), [KMeans(n_clusters=k).fit(data_scaled).inertia_ for k in range(1, 15)], 'r:*')
plt.xlabel('k')
plt.ylabel('Sum of Squared Distances')
plt.title('Elbow Method for Optimal k')
plt.show()



---------------------------- Prac 2 ----------------------------------------------
use Suyog
db.wordcount.insertOne({name:'Suyog Shah'})
db.wordcount.insertOne({name:'Radhe Krishna'})
db.wordcount.mapReduce(
  function(){ this.name.split(" ").forEach(w => emit(w, 1)) },
  function(k, v){ return Array.sum(v) },
  {out: "word"}
)
db.word.find()


---------------------------- prac 3a-1 --------------------------------------------------------------
import pandas as pd, numpy as np, matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
print("Practical performed by Suyog Shah")
df = pd.read_csv("E:/sem2/practicals/bda/homeprices.csv")
print(df)
plt.scatter(df.area, df.price, c='r', label='Actual')
model = LinearRegression().fit(df[['area']], df.price)
plt.plot(df.area, model.predict(df[['area']]), c='b', label='Fit')
plt.xlabel('Area'), plt.ylabel('Price'), plt.legend(), plt.show()
print("Predicted price for 1500 sq ft:", model.predict([[1500]])[0])
print('Model Coefficient:', model.coef_[0])
print('Model Intercept:', model.intercept_)



--------------------------------------- prac 3a-2 --------------------------------------------------------------
import pandas as pd, matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
data = pd.read_csv(r"E:/sem2/practicals/bda/weightwaist.csv - Sheet1.csv")
data.plot.scatter(x='waist_cm', y='weight_kg'); plt.show()
print(data.corr())
lm = LinearRegression().fit(data[['waist_cm']], data['weight_kg'])
print(f"Coef: {lm.coef_}, Intercept: {lm.intercept_}, RÂ²: {lm.score(data[['waist_cm']], data['weight_kg'])}")
Waist_new = pd.DataFrame({'waist_cm': [97]})
Weight_predict = lm.predict(Waist_new)
print(f"Predicted weight for 97cm waist: {Weight_predict[0]}")
df = pd.DataFrame({'waist_cm': [67, 78, 94]})
df['Weight_predicted'] = lm.predict(df)
print(df)
data.plot.scatter(x='waist_cm', y='weight_kg')
plt.plot(data['waist_cm'], lm.predict(data[['waist_cm']]), 'r', linewidth=2)
plt.scatter(Waist_new, Weight_predict, c='k')
plt.show()



---------------------- prac 3a-3 -----------------------------------------
import pandas as pd, matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
df = pd.read_csv("E:/sem2/practicals/bda/canada_per_capita_income.csv")
print(df)
plt.scatter(df['year'], df['per capita income (US$)'], c='g', marker='o')
plt.plot(df['year'], LinearRegression().fit(df[['year']], df['per capita income (US$)']).predict(df[['year']]), 'b')
plt.xlabel('Year'); plt.ylabel('Per Capita Income (US$)'); plt.title('Canada Per Capita Income Over Years'); plt.show()
model = LinearRegression().fit(df[['year']], df['per capita income (US$)'])
print("Predicted income for 2025:", model.predict([[2025]])[0])
print("Model Coefficient:", model.coef_[0])
print("Model Intercept:", model.intercept_)
print("Model Score (R^2):", model.score(df[['year']], df['per capita income (US$)']))



----------------------prac 3b ------------------------------

import numpy as np
from matplotlib import pyplot as plt
from sklearn.linear_model import LogisticRegression 
from sklearn.model_selection import train_test_split 
from sklearn.metrics import confusion_matrix
from sklearn.datasets import make_classification
x, y = make classification(
    n_samples=100,
    n_features=1,
    n_classes=2,
    n_clusters_per_class=1,
    n_informative=1,
    n_redundant=0,
    n_repeated=0,
    random_state=s,
)
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=1)
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)
print("Coefficient (slope):", log_reg.coef_) 
print("Intercept (bias):", log_reg.intercept_)
y_pred= log_reg.predict(x_test)
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
x_values = np.linspace(x.min(), x.max(), 500).reshape(-1, 1)
y_prob = log_reg.predict_proba(x_values) [:, 1]
plt.scatter(x, y, c=y, cmap='plasma', label='Data points') # 'viridis', 'plasma', or 'coolwarm'. 
plt.plot(x_values, y_prob, color='black', label='Decision Boundary(Sigmoid Curve)')
plt.axhline(0.5, color='gray', linestyle='dotted', label= 'Threshold = 0.5')
plt.title("Logistic Regression Decision Boundary")
plt.xlabel('Feature')
plt.ylabel('Probability')
plt.legend() 
plt.show()


----------------------- prac 4a ---------------------------
momgo :---
mongosh
use Suyog1
db.Prac.insertOne({Name:'Suyog',Class:'MScIT',Roll_N0:1})  
db.Prac.insertOne({Name:'Krishnaa',Class:'MScIT',Roll_No:2})  
db.Prac.find() 

py :--
from pymongo import MongoClient

col = MongoClient('localhost:27017').Suyog1.Prac
print("Whole Record: ")
for x in col.find({}, {"_id": 0, "Name": 1, "Class": 1, "Roll_No": 1}): print(x)

print("Inserted ID:", col.insert_one({"Name": "ABC XYZ", "Class": "MSCIT", "Roll_No": 3}).inserted_id)

print("Updated Record: ")
col.update_one({"Name": "ABC XYZ"}, {"$set": {"Class": "BSCIT"}})
for x in col.find({}, {"_id": 0, "Name": 1, "Class": 1, "Roll_No": 1}): print(x)

print("Deleted count:", col.delete_one({"Name": "ABC XYZ"}).deleted_count)
for x in col.find({}, {"_id": 0, "Name": 1, "Class": 1, "Roll_No": 1}): print(x)


--------------------- prac 4b ------------------------
mongo:- 
db.Prac4B.insertMany([
{Name: 'Suyog Shah', Class: 'MScIT', Roll_No: 1}, 
{Name: 'RadheKrishna', Class: 'MScIT', Roll_No: 2}])
db.Prac4B.find()


py code :-
from pymongo import MongoClient

col = MongoClient('localhost:27017').Suyog.Prac4B
print("Whole Record: ")
for x in col.find({}, {"_id": 0, "Name": 1, "Class": 1, "Roll_No": 1}): print(x)

result = col.insert_many([{"Name": "ABC XYZ", "Class": "MSCIT", "Roll_No": 6}, {"Name": "IJK LMN", "Class": "MSCIT", "Roll_No": 7}])
print("Inserted IDs: ", result.inserted_ids)

print("Updated Record: ")
col.update_many({"Class": "MSCIT"}, {"$set": {"Qualification": "Graduated"}})
for x in col.find({}, {"_id": 0, "Name": 1, "Class": 1, "Roll_No": 1}): print(x)

print("Deleted count:", col.delete_many({"Roll_No": {"$in": [6, 7]}}).deleted_count)
for x in col.find({}, {"_id": 0, "Name": 1, "Class": 1, "Roll_No": 1}): print(x)


------------------------- prac 5 --------------------------------
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn import metrics
df = pd.read_csv("E:/sem2/practicals/bda/social.csv")
x, y = df.iloc[:, [2, 3]], df.iloc[:, 4]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=0)
sc_x = StandardScaler()
x_train, x_test = sc_x.fit_transform(x_train), sc_x.transform(x_test)

y_pred = SVC(kernel='linear', random_state=0).fit(x_train, y_train).predict(x_test)
print("x:\n", x, "\ny:\n", y, "\nTraining Data: \n", x_train, "\nTesting Data: \n", x_test)
print("y_pred:\n", y_pred)
print("Accuracy Score with Linear Kernel: ", metrics.accuracy_score(y_test, y_pred))


------------------------- prac 6 --------------------------------
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
df = pd.read_csv("E:/sem2/practicals/bda/Social_Network_Ads.csv")
X, Y = df[["Age", "EstimatedSalary"]], df["Purchased"]
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)
print(df)
print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)

scaler = MinMaxScaler()
X_train_s, X_test_s = scaler.fit_transform(X_train), scaler.transform(X_test)
clf = DecisionTreeClassifier().fit(X_train_s, Y_train)
Y_pred = clf.predict(X_test_s)

X_test_df = pd.DataFrame(X_test_s, columns=["Age", "EstimatedSalary"], index=X_test.index)
plt.scatter(X_test_df[Y_test == 0]["Age"], X_test_df[Y_test == 0]["EstimatedSalary"], c='yellow', alpha=0.7, label="Not Purchased")
plt.scatter(X_test_df[Y_test == 1]["Age"], X_test_df[Y_test == 1]["EstimatedSalary"], c='green', alpha=0.7, label="Purchased")
plt.xlabel("Age (Scaled)"); plt.ylabel("Estimated Salary (Scaled)")
plt.legend(); plt.show()

print("Model Accuracy:", clf.score(X_test_s, Y_test))

---------------------- prac 7 --------------------------------
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
docs = ["The car is fast", "The bike is slow", "The car is fast, bike is slow"]
vec = TfidfVectorizer()
mat = vec.fit_transform(docs).toarray()
print(pd.DataFrame(mat, columns=vec.get_feature_names_out()))


------------------------- prac 8 --------------------------------
import pandas as pd, matplotlib.pyplot as plt, numpy as np, nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import ConfusionMatrixDisplay
df = pd.read_csv("E:/sem2/practicals/bda/Tweets.csv")
print("Reading Data Set\n", df.head(2), df.shape)

df['airline'].value_counts().plot.pie(autopct="%.1f%%", startangle=90, title='Airline Distribution'); plt.show()
df['airline_sentiment'].value_counts().plot.pie(autopct="%.1f%%", startangle=90, title='Sentiment Distribution'); plt.legend(loc='best'); plt.show()

x, y = df['text'], df['airline_sentiment']
xt, xs, yt, ys = train_test_split(x, y, test_size=0.2, random_state=42)
v = CountVectorizer(); xt, xs = v.fit_transform(xt), v.transform(xs)
m = LogisticRegression(max_iter=1000).fit(xt, yt)
ConfusionMatrixDisplay.from_predictions(ys, m.predict(xs), cmap='Blues'); plt.title('Confusion Matrix'); plt.show()
print(f'Accuracy: {np.mean(m.predict(xs)==ys):.2f}')


------------------------- prac 9 --------------------------------



------------------------- prac 10 --------------------------------


















